Speech Enhancement for Hearing-Impaired Patients Using Deep Learning & Real-Time Smartphone Deployment

Authors: Varsha Venkatapathy, Muhammad Patel, Arian Azarang, Virginie Papadopoulou
Affiliation: Lampe Joint Department of Biomedical Engineering, UNC‚ÄìChapel Hill & NC State University
Poster Source: Abrams Scholars Program Spring 2025 

Abrams Scholars Poster Spring 2‚Ä¶

Project Overview

Hearing aids often struggle to suppress background noise, reducing speech clarity for users. This project investigates deep learning-based speech enhancement models with the goal of real-time deployment on smartphones to support hearing-impaired individuals.

We evaluate four architectures (CRNN, CNN, RNN, FCNN) from recent literature and compare their performance using speech quality and intelligibility metrics. The project also includes a prototype Android real-time enhancement app.

Our aim is to identify a model that is both accurate and efficient enough for deployment on consumer mobile devices.

üîä Dataset

Clean speech samples were obtained from a public-domain dataset (TIMIT-style). To create realistic noisy inputs, clean speech was mixed with three noise types:

Babble

Traffic

Machinery

Noise mixtures were created at SNR = ‚àí5, 0, +5 dB, then split 70/20/10 into training, validation, and test sets. 

Abrams Scholars Poster Spring 2‚Ä¶

Model Architectures

We implemented and evaluated four architectures from published studies:

1. CRNN (Convolutional Recurrent Neural Network)

STFT ‚Üí Magnitude Spectrum

Combined convolutional + recurrent layers

Captures both spectral and temporal patterns

Real-time-capable architecture

2. CNN (Convolutional Neural Network)

Frame segmentation ‚Üí FFT ‚Üí Log Power Spectrum

Mel-filterbank + temporal frame concatenation

Efficient for smartphone deployment

Primary model used for full evaluation

3. RNN (Recurrent Neural Network)

Dual-microphone inputs

Extracts real & imaginary STFT components

Recurrent layers model temporal dependencies

4. FCNN (Fully Convolutional Neural Network)

STFT ‚Üí Log-Power Spectrum

Fully convolutional 2D-to-2D mask estimation

Predicts Ideal Ratio Mask (IRM)

All architectural diagrams are shown in the project poster 

Abrams Scholars Poster Spring 2‚Ä¶

.

‚öôÔ∏è Methods: Speech Enhancement Pipeline

Across models, the general preprocessing workflow includes:

Short-Time Fourier Transform (STFT)

Spectral feature extraction

Log-power spectrum

Mel-filterbanks

Real/Imaginary STFT components

Normalization

Context windowing (e.g., 9-frame temporal context for CNN)

Neural network inference

Inverse STFT / mask application

A general SE pipeline diagram is provided in the poster (Fig. 1) 

Abrams Scholars Poster Spring 2‚Ä¶

.

Computing Environment

All experiments were run in Jupyter Notebook on a Windows 10 workstation with:

NVIDIA GeForce RTX 4090 (24 GB, 10,496 CUDA cores)

Intel i9-10850K CPU

128 GB RAM

This enabled stable training of all models, efficient batching, and reproducible experiments. 

Abrams Scholars Poster Spring 2‚Ä¶

Results (CNN Model)

The CNN architecture was trained using:

Optimizer: Adam (1√ó10‚Åª‚Å¥ learning rate)

Batch size: 4

Early stopping: epoch 12/15

~100k audio windows with 9-frame context

Loss Metrics

MSE: ‚Üì from ~0.75 ‚Üí ~0.61

MAE: consistently decreased across epochs

(Figure 7 in the poster shows full training curves.) 

Abrams Scholars Poster Spring 2‚Ä¶

Perceptual Speech Quality

PESQ: ‚Üë from ~1.5 ‚Üí ~1.9 (out of 4.5)

Indicates noticeably clearer, more natural speech

Speech Intelligibility

STOI: ‚Üë from ~0.05 ‚Üí ~0.08

Modest but consistent gains in intelligibility

Spectrogram Analysis

Spectrograms show enhanced speech masks successfully suppressing noise (Fig. 9). 

Abrams Scholars Poster Spring 2‚Ä¶

Conclusion

Attention-enhanced CNN reliably improves clarity and naturalness; further tuning is needed to boost intelligibility. Training for CRNN, RNN, and FCNN models is ongoing.

 Future Work

Optimize the best-performing model for low-memory, low-latency real-time deployment

Test across more diverse noise types & acoustic environments

Conduct user studies with hearing-impaired listeners

Explore integration with hearing aid hardware or mobile hearing-aid apps

Retrain on higher-memory devices with larger batch sizes

Improve STOI via model refinement or hybrid architectures

(From the Future Development section of the poster.) 

Abrams Scholars Poster Spring 2‚Ä¶

 References

All referenced papers are included in the /papers folder. These include:

Real-Time Single-Channel DNN SE (2020)

Real-Time CNN SE for Hearing-Impaired Listeners (2019)

Efficient Two-Microphone RNN SE (2020)

2D-to-2D FCNN Mask Estimation (2020)

(Full citations included in poster.) 

Abrams Scholars Poster Spring 2‚Ä¶

üôè Acknowledgments

Supported by:

UNC Office of Undergraduate Research (OUR)

BME Abrams Scholars Program (UNC/NCSU)
